%!TEX root = std.tex
\section*{Motivation}

Enable implementations of \tcode{atomic_ref} which do not require a global lock array
while perserving current behavior as much as possible.

\section*{Proposed Wording}

\textbf{\textit{This font is used to provide guidance to the editors.}} \\

\textbf{\textit{Make the following changes in [atomics.ref.generic].}} \\

\indexlibrary{\idxcode{atomic_ref}}%
\indexlibrarymember{value_type}{atomic_ref}%

\begin{addedblock}
\begin{codeblock}
namespace std {
  struct atomic_ref_assume_lock_free_t    {};
  struct atomic_ref_perfer_user_lock_t    {};
  
  inline constexpr atomic_ref_assume_lock_free_t  atomic_ref_assume_lock_free {};
  inline constexpr atomic_ref_perfer_user_lock_t  atomic_ref_perfer_user_lock {};

  struct atomic_ref_assume_no_user_locks {};
}
\end{codeblock}
\end{addedblock}


\begin{codeblock}
namespace std {

  template<class T@\added{, class Lock = mutex}@> struct atomic_ref {
  private:
    T* ptr;      // \expos
    @\added{Lock* ulock; // \expos}@
  public:
    using value_type = T;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;
\end{codeblock}
\begin{removedblock}
\begin{codeblock}
    explicit atomic_ref(T&) noexcept;
\end{codeblock}
\end{removedblock}
\begin{addedblock}
\begin{codeblock}
    explicit atomic_ref(T&) noexcept(!is_same_v<Lock,atomic_ref_assume_no_user_locks> 
      || @\impdefx{can an implementation support \tcode{atomic_ref} without a user provided lock}@);
\end{codeblock}
\end{addedblock}
\begin{codeblock}
    @\added{\tcode{atomic_ref(T\&, atomic_ref_assume_lock_free_t) noexcept;}}@
   
    @\added{\tcode{atomic_ref(T\&, Lock\&) noexcept;}}@
    @\added{\tcode{atomic_ref(T\&, Lock\&, atomic_ref_perfer_user_lock_t) noexcept;}}@
    
    @\added{\tcode{atomic_ref(T\&, ranges::RandomAccessRange<Lock>) noexcept;}}@
    @\added{\tcode{atomic_ref(T\&, ranges::RandomAccessRange<Lock>, atomic_ref_perfer_user_lock_t) noexcept;}}@

    atomic_ref(const atomic_ref&) noexcept;

    T operator=(T) const noexcept;
    operator T() const noexcept;

    bool is_lock_free() const noexcept;
    void store(T, memory_order = memory_order_seq_cst) const noexcept;
    T load(memory_order = memory_order_seq_cst) const noexcept;
    T exchange(T, memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order = memory_order_seq_cst) const noexcept;
  };
}
\end{codeblock}
~\\

\textbf{\textit{Make the following changes in [atomics.ref.operations].}} \\

\begin{itemdecl}
static constexpr bool is_always_lock_free;
\end{itemdecl}

\begin{itemdescr}
\pnum
The static data member \tcode{is_always_lock_free} is \tcode{true}
if the \tcode{atomic_ref} type's operations are always lock-free
\added{on objects aligned to \tcode{required_lock_free_alignment}},
and \tcode{false} otherwise.
\end{itemdescr}

\begin{itemdecl}
static constexpr size_t required_@\added{lock_free_}@alignment;
\end{itemdecl}

\begin{itemdescr}
\pnum
The alignment required for an object to be referenced \added{lock-free} by an atomic reference,
which is at least \tcode{alignof(T)}.

\pnum
\begin{note}
Hardware could require an object
referenced by an \tcode{atomic_ref}
to have stricter alignment\iref{basic.align}
than other objects of type \tcode{T}.
Further, whether operations on an \tcode{atomic_ref}
are lock-free could depend on the alignment of the referenced object.
For example, lock-free operations on \tcode{std::complex<double>}
could be supported only if aligned to \tcode{2*alignof(double)}.
\end{note}
\end{itemdescr}

\begin{addedblock}
\begin{itemdecl}
static constexpr bool never_requires_user_lock;
\end{itemdecl}

\begin{itemdescr}
\pnum
Is \tcode{true} if an implementation never requires the user to provide a 
lock for objects of type \tcode{T} and \tcode{false} otherwise.
\end{itemdescr}

\begin{itemdecl}
static is_lock_free(T& obj); noexcept
\end{itemdecl}

\begin{itemdescr}
\returns Returns \tcode{true} if atomic operations on the object referenced by
\tcode{obj} can be lock-free or if the \tcode{Lock} type is equivalent to 
\tcode{atomic_ref_assume_lock_free_t}.
\end{itemdescr}

\begin{itemdecl}
static requires_user_lock(T& obj); noexcept
\end{itemdecl}

\begin{itemdescr}
\returns Returns \tcode{false} if the \tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}
or \tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
Otherwise, returns \tcode{true} if \tcode{atomic_ref} requires the user to
provide a valid pointer to a \tcode{Lock} object when constructing an 
\tcode{atomic_ref} from \tcode{obj}. 
\end{itemdescr}
\end{addedblock}

\begin{itemdecl}
\begin{removedblock}
explicit atomic_ref(T&) noexcept;
\end{removedblock}
\begin{addblock}
explicit atomic_ref(T&) noexcept(!is_same_v<Lock,atomic_ref_assume_no_user_locks>
  || @\impdefx{can the implementation support \tcode{atomic_ref} without a user provided lock}@);
\end{addblock}
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires The referenced object shall be aligned to \tcode{required_lock_free_alignment}}.
\begin{addedblock}
\expects \\
\tcode{requires_user_lock(obj)} is \tcode{false}, and this constructor is used for all \tcode{atomic_ref} objects which 
reference the object referenced by \tcode{obj} concurrently.
\end{addedblock}

\pnum
\effects \removed{Constructs an atomic reference that references the object.}\\
\begin{addedblock}Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = nullptr;
\end{codeblock}
\end{addedblock}

\pnum
\throws \removed{Nothing}
\begin{addedblock}
\impdefx{}.  If \tcode{requires_user_lock{obj}} could be \tcode{true}, an implementation
throws \tcode{???_error} if \tcode{requires_user_lock} is \tcode{true};
\end{addedblock}
\end{itemdescr}

\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, atomic_ref_assume_lock_free_t);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects The object referenced \tcode{obj} is be aligned to \tcode{required_lock_free_alignment} and
this constructor is used for all \tcode{atomic_ref} objects which reference the 
object referenced by \tcode{obj} concurrently. 

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = nullptr;
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, Lock& lk);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements 
or is equivalent to \tcode{atomic_ref_assume_lock_free_t}.
For all \tcode{atomic_ref} objects that concurrently reference the object referenced
by \tcode{obj} either \tcode{requires_user_lock(obj)} is \tcode{false} or
the following conditions are true:

\begin{itemize}
\item \tcode{lk} references the same \tcode{Lock} object, and
\item \tcode this constructor is used for all the \tcode{atomic_ref} objects mentioned above.
\end{itemize}

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = requires_user_lock(obj) ? &lk : nullptr;
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, Lock& lk, atomic_ref_perfer_user_lock_t);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements,
is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or \tcode{requires_user_lock(obj)} 
is \tcode{false}.
For all \tcode{atomic_ref} objects that concurrently reference the object referenced
by \tcode{obj} both of the following conditions are true:

\begin{itemize}
\item \tcode{lk} references the same \tcode{Lock} object, and
\item \tcode this constructor is used for all the \tcode{atomic_ref} objects mentioned above.
\end{itemize}

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = is_lock_free(obj) ? nullptr : &lk;
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, ranges::RandomAccessRange<Lock> lks);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or \tcode{requires_user_lock(obj)} 
is \tcode{false}.
For all \tcode{atomic_ref} objects that concurrently reference the object referenced
by \tcode{obj} either \tcode{requires_user_lock(obj)} is \tcode{false} or
the following conditions are true:

\begin{itemize}
\item \tcode{lks} is a non-empty range of \tcode{Lock} objects,
\item \tcode{lks} represents equivalent ranges of \tcode{Lock} objects, and
\item \tcode this constructor is used for all the \tcode{atomic_ref} objects mentioned above.
\end{itemize}

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = requires_user_lock(obj)) ? ranges::begin(lks)[std::hash(&obj)@ranges::size(lks)] : nullptr;
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, ranges::RandomAccessRange<Lock> lks, atomic_ref_perfer_user_lock_t);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements 
or is equivalent to \tcode{atomic_ref_assume_lock_free_t}.
For all \tcode{atomic_ref} objects that concurrently reference the object referenced
by \tcode{obj} all of the following conditions are true:

\begin{itemize}
\item \tcode{lks} is a non-empty range of \tcode{Lock} objects,
\item \tcode{lks} represents equivalent ranges of \tcode{Lock} objects, and
\item \tcode this constructor is used for all the \tcode{atomic_ref} objects mentioned above.
\end{itemize}

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = is_lock_free(obj) ? nullptr : ranges::begin(lks)[std::hash(&obj)@ranges::size(lks)];
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{itemdecl}
atomic_ref(const atomic_ref& ref) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \removed{Constructs an atomic reference
that references the object referenced by \tcode{ref}.}
\begin{addedblock}Equivalent to:
\begin{codeblock}
  ptr = ref.obj;
  ulock = ref.ulock;
\end{codeblock}
\end{addedblock}

\end{itemdescr}


\begin{itemdecl}
void store(T desired, memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{order} argument shall not be
\tcode{memory_order_consume},
\tcode{memory_order_acquire}, nor
\tcode{memory_order_acq_rel}.

\pnum
\removed{Atomically replaces the value referenced by \tcode{*ptr}
with the value of \tcode{desired}}.
\begin{addedblock}
\effects If \tcode{ulock} is equivalent to \tcode{nullptr}
then the operation is equivalent to atomically performing the following:
\begin{codeblock}
  memcpy(ptr, &desired, sizeof(T));
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  ulock->ulock();
  memcpy(ptr, &desired, sizeof(T));
  ulock->unlock();
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\begin{itemdecl}
T load(memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{order} argument shall not be
\tcode{memory_order_release} nor \tcode{memory_order_acq_rel}.

\begin{addedblock}
\pnum
\effects If \tcode{ulock} is equivalent to \tcode{nullptr} then the
operation is equivalent to atomically performing the following:
\begin{codeblock}
  T result;
  memcpy(&result, ptr, sizeof(T));
  return result;
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  T result;
  ulock->ulock();
  memcpy(&result, ptr, sizeof(T));
  ulock->unlock();
  return result;
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.

\removed{
\pnum
\returns Atomically returns the value referenced by \tcode{*ptr}.
}
\end{itemdescr}

\begin{itemdecl}
T exchange(T desired, memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{Atomically replaces the value referenced by \tcode{*ptr}
with \tcode{desired}.}
\begin{addedblock}
\effects \added{If \tcode{ulock} is equivalent to \tcode{nullptr}}
then the operation is equivalent to atomically performing the following:
\begin{codeblock}
  T result;
  memcpy(&result, ptr, sizeof(T));
  memcpy(ptr, &desired, sizeof(T));
  return result;
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  T result;
  ulock->ulock();
  memcpy(&result, ptr, sizeof(T));
  memcpy(ptr, &desired, sizeof(T));
  ulock->unlock();
  return result;
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.
This operation is an atomic read-modify-write operation\iref{intro.multithread}.

\begin{removedblock}
\pnum
\returns Atomically returns the value referenced by \tcode{*ptr}
immediately before the effects.
\end{removedblock}
\end{itemdescr}

\begin{itemdecl}
bool compare_exchange_weak(T& expected, T desired,
                           memory_order success, memory_order failure) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order success, memory_order failure) const noexcept;

bool compare_exchange_weak(T& expected, T desired,
                           memory_order order = memory_order_seq_cst) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{failure} argument shall not be
\tcode{memory_order_release} nor \tcode{memory_order_acq_rel}.

\pnum
\effects
\begin{addedblock}
When only one \tcode{memory_order} argument is supplied,
the value of \tcode{success} is \tcode{order}, and
the value of \tcode{failure} is \tcode{order}
except that a value of \tcode{memory_order_acq_rel} shall be replaced by
the value \tcode{memory_order_acquire} and
a value of \tcode{memory_order_release} shall be replaced by
the value \tcode{memory_order_relaxed}.
\end{addedblock}

\begin{addedblock}
If \tcode{ulock} is equivalent to \tcode{nullptr} then
the operation is equivalent to atomically performing the following:
\begin{codeblock}
  T old;
  memcpy(&old, ptr, sizeof(T));
  bool result = 0 == memcmp(&expected, &old, sizeof(T));
  if (result) memcpy(ptr, &desired, sizeof(T));
  else memcpy(&expected, &old, sizeof(T));
  return result;
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  T old;
  ulock->ulock()
  memcpy(&old, ptr, sizeof(T));
  bool result = 0 == memcmp(&expected, &old, sizeof(T));
  if (result) memcpy(ptr, &desired, sizeof(T));
  else memcpy(&expected, &old, sizeof(T));
  ulock->unlock();
  return result;
\end{codeblock}

\pnum
Let \tcode{R} be the return value of the operation.
If and only if \tcode{R} is \tcode{true},
memory is affected according to the value of \tcode{success}, and
if \tcode{R} is \tcode{false},
memory is affected according to the value of \tcode{failure}.
\end{addedblock}

\begin{removedblock}
Retrieves the value in \tcode{expected}.
It then atomically compares the value representation of
the value referenced by \tcode{*ptr} for equality
with that previously retrieved from \tcode{expected},
and if \tcode{true}, replaces the value referenced by \tcode{*ptr}
with that in \tcode{desired}.
When only one \tcode{memory_order} argument is supplied,
the value of \tcode{success} is \tcode{order}, and
the value of \tcode{failure} is \tcode{order}
except that a value of \tcode{memory_order_acq_rel} shall be replaced by
the value \tcode{memory_order_acquire} and
a value of \tcode{memory_order_release} shall be replaced by
the value \tcode{memory_order_relaxed}.
If and only if the comparison is \tcode{false} then,
after the atomic operation,
the value in \tcode{expected} is replaced by
the value read from the value referenced by \tcode{*ptr}
during the atomic comparison.
\end{removedblock}
If \removed{the operation returns}\added{\tcode{R} is} \tcode{true},
these operations are atomic read-modify-write operations\iref{intro.races}
on the value referenced by \tcode{*ptr}.
Otherwise, these operations are atomic load operations on that memory.

\begin{removedblock}
\pnum
\returns The result of the comparison.
\end{removedblock}

\pnum
\remarks A weak compare-and-exchange operation may fail spuriously.
That is, even when the contents of memory referred to
by \tcode{expected} and \tcode{ptr} are equal,
it may return \tcode{false} and
store back to \tcode{expected} the same memory contents
that were originally there.
\begin{note}
This spurious failure enables implementation of compare-and-exchange
on a broader class of machines, e.g., load-locked store-conditional machines.
A consequence of spurious failure is
that nearly all uses of weak compare-and-exchange will be in a loop.
When a compare-and-exchange is in a loop,
the weak version will yield better performance on some platforms.
When a weak compare-and-exchange would require a loop and
a strong one would not, the strong one is preferable.
\end{note}
\end{itemdescr}

~\\
~\\

\textbf{\textit{Make the following changes in [atomics.ref.int].}} \\

\begin{codeblock}
namespace std {
  template<@\added{Lock}@> struct atomic_ref<@\placeholder{integral}\added{, Lock}@> {
  private:
    @\placeholder{integral}@* ptr;        // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = @\placeholder{integral}@;
    using difference_type = value_type;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(@\placeholder{integral}@&) @\added{noexcept(\impdefx{can an implementation support \tcode{atomic_ref} without a user provided lock})}@;
    @\added{atomic_ref(\placeholder{integral}\&, atomic_ref_assume_lock_free_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{integral}\&, Lock\&) noexcept;}@
    @\added{atomic_ref(\placeholder{integral}\&, Lock\&, atomic_ref_perfer_user_lock_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{integral}\&, ranges::RandomAccessRange<Lock>) noexcept;}@
    @\added{atomic_ref(\placeholder{integral}\&, ranges::RandomAccessRange<Lock>, atomic_ref_perfer_user_lock_t) noexcept;}@

    atomic_ref(const atomic_ref&) noexcept;

    @\placeholdernc{integral}@ operator=(@\placeholder{integral}@) const noexcept;
    operator @\placeholdernc{integral}@() const noexcept;

    bool is_lock_free() const noexcept;
    void store(@\placeholdernc{integral}@, memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ load(memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ exchange(@\placeholdernc{integral}@,
                      memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order = memory_order_seq_cst) const noexcept;

    @\placeholdernc{integral}@ fetch_add(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_sub(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_and(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_or(@\placeholdernc{integral}@,
                      memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_xor(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;

    @\placeholdernc{integral}@ operator++(int) const noexcept;
    @\placeholdernc{integral}@ operator--(int) const noexcept;
    @\placeholdernc{integral}@ operator++() const noexcept;
    @\placeholdernc{integral}@ operator--() const noexcept;
    @\placeholdernc{integral}@ operator+=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator-=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator&=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator|=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator^=(@\placeholdernc{integral}@) const noexcept;
  };
}
\end{codeblock}

\textbf{\textit{Make the following changes in [atomics.ref.float].}} \\

\begin{codeblock}
namespace std {
  template<@\added{Lock}@> struct atomic_ref<@\placeholder{floating-point}\added{, Lock}@> {
  private:
    @\placeholder{floating-point}@* ptr;  // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = @\placeholder{floating-point}@;
    using difference_type = value_type;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(@\placeholder{floating-point}@&) @\added{noexcept(\impdefx{can an implementation support \tcode{atomic_ref} without a user provided lock})}@;
    @\added{atomic_ref(\placeholder{floating-point}\&, atomic_ref_assume_lock_free_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{floating-point}\&, Lock\&) noexcept;}@
    @\added{atomic_ref(\placeholder{floating-point}\&, Lock\&, atomic_ref_perfer_user_lock_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{floating-point}\&, ranges::RandomAccessRange<Lock>) noexcept;}@
    @\added{atomic_ref(\placeholder{floating-point}\&, ranges::RandomAccessRange<Lock>, atomic_ref_perfer_user_lock_t) noexcept;}@

    atomic_ref(const atomic_ref&) noexcept;

    @\placeholder{floating-point}@ operator=(@\placeholder{floating-point}@) noexcept;
    operator @\placeholdernc{floating-point}@() const noexcept;

    bool is_lock_free() const noexcept;
    void store(@\placeholder{floating-point}@, memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ load(memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ exchange(@\placeholder{floating-point}@,
                            memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                                 memory_order = memory_order_seq_cst) const noexcept;

    @\placeholder{floating-point}@ fetch_add(@\placeholder{floating-point}@,
                             memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ fetch_sub(@\placeholder{floating-point}@,
                             memory_order = memory_order_seq_cst) const noexcept;

    @\placeholder{floating-point}@ operator+=(@\placeholder{floating-point}@) const noexcept;
    @\placeholder{floating-point}@ operator-=(@\placeholder{floating-point}@) const noexcept;
  };
}
\end{codeblock}

\textbf{\textit{Make the following changes in [atomics.ref.pointer].}} \\

\begin{codeblock}
namespace std {
  template<class T@\added{, Lock}@> struct atomic_ref<T*@\added{, Lock}@> {
  private:
    T** ptr;              // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = T*;
    using difference_type = ptrdiff_t;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(T*) @\added{noexcept(\impdefx{can an implementation support \tcode{atomic_ref} without a user provided lock})}@;
    @\added{atomic_ref(T*, atomic_ref_assume_lock_free_t) noexcept;}@
   
    @\added{atomic_ref(T*, Lock\&) noexcept;}@
    @\added{atomic_ref(T*, Lock\&, atomic_ref_perfer_user_lock_t) noexcept;}@
    
    @\added{atomic_ref(T*, ranges::RandomAccessRange<Lock>) noexcept;}@
    @\added{atomic_ref(T*, ranges::RandomAccessRange<Lock>, atomic_ref_perfer_user_lock_t) noexcept;}@

    atomic_ref(const atomic_ref&) noexcept;

    T* operator=(T*) const noexcept;
    operator T*() const noexcept;

    bool is_lock_free() const noexcept;
    void store(T*, memory_order = memory_order_seq_cst) const noexcept;
    T* load(memory_order = memory_order_seq_cst) const noexcept;
    T* exchange(T*, memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order = memory_order_seq_cst) const noexcept;

    T* fetch_add(difference_type, memory_order = memory_order_seq_cst) const noexcept;
    T* fetch_sub(difference_type, memory_order = memory_order_seq_cst) const noexcept;

    T* operator++(int) const noexcept;
    T* operator--(int) const noexcept;
    T* operator++() const noexcept;
    T* operator--() const noexcept;
    T* operator+=(difference_type) const noexcept;
    T* operator-=(difference_type) const noexcept;
  };
}
\end{codeblock}

