%!TEX root = std.tex
\section*{Motivation}

Enable implementations of \tcode{atomic_ref} which do not require a global lock array
while perserving current behavior as much as possible.

\section*{Proposed Wording}

\textbf{\textit{This font is used to provide guidance to the editors.}} \\

\textbf{\textit{Make the following changes in [atomics.ref.generic].}} \\

\indexlibrary{\idxcode{atomic_ref}}%
\indexlibrarymember{value_type}{atomic_ref}%

\begin{addedblock}
\begin{codeblock}
namespace std {
  struct atomic_ref_assume_lock_free_t    {};
  struct atomic_ref_assume_no_user_lock_t {};
  struct atomic_ref_prefer_user_lock_t    {};
  
  inline constexpr atomic_ref_assume_lock_free_t  atomic_ref_assume_lock_free {};
  inline constexpr atomic_ref_prefer_user_lock_t  atomic_ref_prefer_user_lock {};
}
\end{codeblock}
\end{addedblock}


\begin{codeblock}
namespace std {

  template<class T@\added{, class LockT=\unspec}@> struct atomic_ref {
  private:
    T* ptr;      // \expos
    @\added{Lock* ulock; // \expos}@
  public:
    @\added{using Lock = LockT;}@
    using value_type = T;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(T&) noexcept;
    @\added{\tcode{atomic_ref(T\&, atomic_ref_assume_lock_free_t) noexcept;}}@
   
    @\added{\tcode{atomic_ref(T\&, Lock\&) noexcept;}}@
    @\added{\tcode{atomic_ref(T\&, Lock\&, atomic_ref_prefer_user_lock_t) noexcept;}}@
    
    @\added{\tcode{template<class Select> atomic_ref(T\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{Select = \unspec ) noexcept;}}@
    @\added{\tcode{template<class Select> atomic_ref(T\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{atomic_ref_prefer_user_lock_t, Select = \unspec ) noexcept;}}@

    atomic_ref(const atomic_ref&) noexcept;

    T operator=(T) const noexcept;
    operator T() const noexcept;

    bool is_lock_free() const noexcept;
    void store(T, memory_order = memory_order_seq_cst) const noexcept;
    T load(memory_order = memory_order_seq_cst) const noexcept;
    T exchange(T, memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order = memory_order_seq_cst) const noexcept;
  };
}
\end{codeblock}
~\\

\textbf{\textit{Make the following changes in [atomics.ref.operations].}} \\

\begin{addedblock}
\tcode{atomic_ref} instances referencing the same value of ptr and ulock are 
called \textit{equivalent}. Concurrent access to the same value through 
equivalent \tcode{atomic_ref} instances does not create a data race. 
\begin{note} Concurrent access to the value directly, or through a non-equivalent
\tcode{atomic_ref} instance, may introduce a data race.
\end{note}
\end{addedblock}

\begin{itemdecl}
static constexpr bool is_always_lock_free;
\end{itemdecl}

\begin{itemdescr}
\pnum
The static data member \tcode{is_always_lock_free} is \tcode{true}
if the \tcode{atomic_ref} type's operations are always lock-free
\added{on objects aligned to \tcode{required_lock_free_alignment}},
and \tcode{false} otherwise.
\end{itemdescr}


\begin{itemdecl}
static constexpr size_t required_@\added{lock_free_}@alignment;
\end{itemdecl}

\begin{itemdescr}
\pnum
The alignment required for an object to be referenced \added{lock-free} by an atomic reference,
which is at least \tcode{alignof(T)}.

\pnum
\begin{note}
Hardware could require an object
referenced by an \tcode{atomic_ref}
to have stricter alignment\iref{basic.align}
than other objects of type \tcode{T}.
Further\added{more}, whether operations on an \tcode{atomic_ref}
are lock-free could depend on the alignment of the referenced object.
For example, lock-free operations on \tcode{std::complex<double>}
could be supported only if aligned to \tcode{2*alignof(double)}.
\end{note}
\end{itemdescr}


\begin{addedblock}
\begin{itemdecl}
static constexpr bool never_requires_user_lock;
\end{itemdecl}

\begin{itemdescr}
\pnum
Is \tcode{true} if an implementation never requires the user to provide a 
lock for objects of type \tcode{T} and \tcode{false} otherwise.
\end{itemdescr}


\begin{itemdecl}
static is_lock_free(T& obj) noexcept;
\end{itemdecl}

\begin{itemdescr}
\returns Returns \tcode{true} if atomic operations on the object referenced by
\tcode{obj} can be lock-free or if the \tcode{Lock} type is equivalent to 
\tcode{atomic_ref_assume_lock_free_t}.
\end{itemdescr}


\begin{itemdecl}
static requires_user_lock(T& obj) noexcept;
\end{itemdecl}

\begin{itemdescr}
\returns Returns \tcode{false} if the type \tcode{Lock} is equivalent to either
\tcode{atomic_ref_assume_lock_free_t} or \tcode{atomic_ref_assume_no_user_lock_t}.
Otherwise, returns \tcode{true} if \tcode{atomic_ref} requires the user to
provide a valid reference to a \tcode{Lock} object 
or a non-empty \tcode{ranges::RandomAccessRange<Lock>} 
when constructing an \tcode{atomic_ref} from \tcode{obj}. 
\end{itemdescr}
\end{addedblock}


\begin{itemdecl}
explicit atomic_ref(T& obj) noexcept@\added{(\seebelow)}@;
\end{itemdecl}

\begin{itemdescr}
\begin{addedblock}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively.
\end{addedblock}

\pnum
\removed{\requires The referenced object shall be aligned to \tcode{required_lock_free_alignment}}.
\begin{addedblock}
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \\ \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
\\ Furthermore, \tcode{requires_user_lock(obj)} is \tcode{false}.
\end{addedblock}

\pnum
\effects \removed{Constructs an atomic reference that references the object.}\\
\begin{addedblock}Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = nullptr;
\end{codeblock}
\end{addedblock}

\pnum
\throws Nothing.
 
\pnum
\begin{addedblock}
\remarks Calls \tcode{terminate()} if \tcode{requires_user_lock(obj)} is \tcode{true}.
\end{addedblock}
\end{itemdescr}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, atomic_ref_assume_lock_free_t);
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively.

\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
\\ Furthermore, the object referenced \tcode{obj} is be aligned to \tcode{required_lock_free_alignment}.

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = nullptr;
\end{codeblock}

\pnum
\throws Nothing.

\pnum
\remarks Calls \tcode{terminate} if \tcode{is_lock_free(obj)} is \tcode{false}.

\end{itemdescr}
\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, Lock& lk);
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively.

\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
\\ Furthermore, either \tcode{requires_user_lock(obj)} is false or 
all \tcode{atomic_ref} objects which concurrently reference the object referenced
by \tcode{obj} are equivalent.

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = requires_user_lock(obj) ? &lk : nullptr;
\end{codeblock}

\pnum
\throws Nothing.

\end{itemdescr}
\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
atomic_ref(T& obj, Lock& lk, atomic_ref_prefer_user_lock_t);
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively.

\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
\\ Furthermore, all \tcode{atomic_ref} objects which concurrently reference the 
object referenced by \tcode{obj} are equivalent.

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = is_lock_free(obj) ? nullptr : &lk;
\end{codeblock}

\pnum
\throws Nothing.
\end{itemdescr}

\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
template <class Select>
atomic_ref(T& obj, ranges::RandomAccessRange<Lock> lks, 
  Select sel = @\unspec@ );
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively. Furthermore, \\
\tcode{INVOKE(sel(const T\&, ranges::RandomAccessRange<Lock>))} returns
a reference to an object of type \tcode{Lock}.

\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
Also, the range \tcode{lks} is non-empty and \tcode{INVOKE(sel(obj, lks))} returns a
reference to a \tcode{Lock} object in the range of \tcode{lks}. 
Furthermore, either \tcode{require} is \tcode{false} or
for all \tcode{atomic_ref} objects which concurrently reference the object referenced
by \tcode{obj} are equivalent.

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = requires_user_lock(obj) ? 
    std::addressof(INVOKE(sel(obj, lks))) : nullptr;
\end{codeblock}

\pnum
\throws Nothing.

\pnum
\remarks Calls \tcode{terminate} if the range \tcode{lks} is empty.
\tcode{requires_user_lock(obj)} is \tcode{true}.

\end{itemdescr}
\end{addedblock}


\begin{addedblock}
\begin{itemdecl}
template <class Select>
atomic_ref(T& obj, ranges::RandomAccessRange<Lock> lks,
  atomic_ref_prefer_user_lock_t, Sel sel = @\unspec@ );
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates If type \tcode{Lock} is either equivalent to \tcode{atomic_ref_assume_lock_free_t} 
or \\ \tcode{atomic_ref_assume_no_user_lock_t} then the implementation can possibly be
lock-free or not require a user provided lock for some objects of type \tcode{T} 
respectively. Furthermore, \\
\tcode{INVOKE(sel(const T\&, ranges::RandomAccessRange<Lock>))} returns
a reference to an object of type \tcode{Lock}.

\pnum
\expects The type \tcode{Lock} either meets the \oldconcept{BasicLockable} requirements, 
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_lock_free_t}, or
\tcode{Lock} is equivalent to \tcode{atomic_ref_assume_no_user_lock_t}.
Also, the range \tcode{lks} is non-empty and \tcode{INVOKE(sel(obj, lks))} returns a
reference to a \tcode{Lock} object in the range of \tcode{lks}. 
Furthermore, all \tcode{atomic_ref} objects which concurrently reference the object referenced
by \tcode{obj} are equivalent.

\pnum
\effects Equivalent to:
\begin{codeblock}
  ptr = &obj;
  ulock = is_lock_free(obj) ? 
    nullptr: std::addressof(INVOKE(sel(obj, lks)));
\end{codeblock}

\pnum
\throws Nothing.

\pnum
\remarks Calls \tcode{terminate} if the range \tcode{lks} is empty and 
\tcode{is_lock_free(obj)} is \tcode{false}.

\end{itemdescr}
\end{addedblock}


\begin{itemdecl}
atomic_ref(const atomic_ref& ref) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \removed{Constructs an atomic reference
that references the object referenced by \tcode{ref}.}
\begin{addedblock}Equivalent to:
\begin{codeblock}
  ptr = ref.obj;
  ulock = ref.ulock;
\end{codeblock}
\end{addedblock}

\end{itemdescr}


\begin{itemdecl}
void store(T desired, memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{order} argument shall not be
\tcode{memory_order_consume},
\tcode{memory_order_acquire}, nor
\tcode{memory_order_acq_rel}.

\pnum
\removed{Atomically replaces the value referenced by \tcode{*ptr}
with the value of \tcode{desired}}.
\begin{addedblock}
\effects If \tcode{ulock} is equivalent to \tcode{nullptr}
then the operation is equivalent to atomically performing the following:
\begin{codeblock}
  memcpy(ptr, &desired, sizeof(T));
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  ulock->lock();
  memcpy(ptr, &desired, sizeof(T));
  ulock->unlock();
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.
\end{itemdescr}


\begin{itemdecl}
T load(memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{order} argument shall not be
\tcode{memory_order_release} nor \tcode{memory_order_acq_rel}.

\begin{addedblock}
\pnum
\effects If \tcode{ulock} is equivalent to \tcode{nullptr} then the
operation is equivalent to atomically performing the following:
\begin{codeblock}
  alignas(T) char result[sizeof(T)];
  memcpy(result, ptr, sizeof(T));
  return *reinterpret_cast<T*>(result);
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  alignas(T) char result[sizeof(T)];
  ulock->lock();
  memcpy(result, ptr, sizeof(T));
  ulock->unlock();
  return *reinterpret_cast<T*>(result);
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.

\removed{
\pnum
\returns Atomically returns the value referenced by \tcode{*ptr}.
}
\end{itemdescr}


\begin{itemdecl}
T exchange(T desired, memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{Atomically replaces the value referenced by \tcode{*ptr}
with \tcode{desired}.}
\begin{addedblock}
\effects \added{If \tcode{ulock} is equivalent to \tcode{nullptr}}
then the operation is equivalent to atomically performing the following:
\begin{codeblock}
  alignas(T) char result[sizeof(T)];
  memcpy(result, ptr, sizeof(T));
  memcpy(ptr, &desired, sizeof(T));
  return *reinterpret_cast<T*>(result);
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  alignas(T) char result[sizeof(T)];
  ulock->lock();
  memcpy(result, ptr, sizeof(T));
  memcpy(ptr, &desired, sizeof(T));
  ulock->unlock();
  return *reinterpret_cast<T*>(result);
\end{codeblock}
\end{addedblock}
Memory is affected according to the value of \tcode{order}.
This operation is an atomic read-modify-write operation\iref{intro.multithread}.

\begin{removedblock}
\pnum
\returns Atomically returns the value referenced by \tcode{*ptr}
immediately before the effects.
\end{removedblock}
\end{itemdescr}


\begin{itemdecl}
bool compare_exchange_weak(T& expected, T desired,
                           memory_order success, memory_order failure) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order success, memory_order failure) const noexcept;

bool compare_exchange_weak(T& expected, T desired,
                           memory_order order = memory_order_seq_cst) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\removed{\requires}\added{\expects} The \tcode{failure} argument shall not be
\tcode{memory_order_release} nor \tcode{memory_order_acq_rel}.

\pnum
\effects
\begin{addedblock}
When only one \tcode{memory_order} argument is supplied,
the value of \tcode{success} is \tcode{order}, and
the value of \tcode{failure} is \tcode{order}
except that a value of \tcode{memory_order_acq_rel} shall be replaced by
the value \tcode{memory_order_acquire} and
a value of \tcode{memory_order_release} shall be replaced by
the value \tcode{memory_order_relaxed}.
\end{addedblock}

\begin{addedblock}
If \tcode{ulock} is equivalent to \tcode{nullptr} then
the operation is equivalent to atomically performing the following:
\begin{codeblock}
  alignas(T) char old[sizeof(T)];
  memcpy(old, ptr, sizeof(T));
  bool result = 0 == memcmp(&expected, old, sizeof(T));
  if (result) memcpy(ptr, &desired, sizeof(T));
  else memcpy(&expected, old, sizeof(T));
  return result;
\end{codeblock}
Otherwise, equivalent to:
\begin{codeblock}
  alignas(T) char old[sizeof(T)];
  ulock->lock()
  memcpy(old, ptr, sizeof(T));
  bool result = 0 == memcmp(&expected, old, sizeof(T));
  if (result) {
    memcpy(ptr, &desired, sizeof(T));
    ulock->unlock();
  } else {
    ulock->unlock();
    memcpy(&expected, old, sizeof(T));
  }
  return result;
\end{codeblock}

\pnum
Let \tcode{R} be the return value of the operation.
If and only if \tcode{R} is \tcode{true},
memory is affected according to the value of \tcode{success}, and
if \tcode{R} is \tcode{false},
memory is affected according to the value of \tcode{failure}.
\end{addedblock}

\begin{removedblock}
Retrieves the value in \tcode{expected}.
It then atomically compares the value representation of
the value referenced by \tcode{*ptr} for equality
with that previously retrieved from \tcode{expected},
and if \tcode{true}, replaces the value referenced by \tcode{*ptr}
with that in \tcode{desired}.
When only one \tcode{memory_order} argument is supplied,
the value of \tcode{success} is \tcode{order}, and
the value of \tcode{failure} is \tcode{order}
except that a value of \tcode{memory_order_acq_rel} shall be replaced by
the value \tcode{memory_order_acquire} and
a value of \tcode{memory_order_release} shall be replaced by
the value \tcode{memory_order_relaxed}.
If and only if the comparison is \tcode{false} then,
after the atomic operation,
the value in \tcode{expected} is replaced by
the value read from the value referenced by \tcode{*ptr}
during the atomic comparison.
\end{removedblock}
If \removed{the operation returns}\added{\tcode{R} is} \tcode{true},
these operations are atomic read-modify-write operations\iref{intro.races}
on the value referenced by \tcode{*ptr}.
Otherwise, these operations are atomic load operations on that memory.

\begin{removedblock}
\pnum
\returns The result of the comparison.
\end{removedblock}

\pnum
\remarks A weak compare-and-exchange operation may fail spuriously.
That is, even when the contents of memory referred to
by \tcode{expected} and \tcode{ptr} are equal,
it may return \tcode{false} and
store back to \tcode{expected} the same memory contents
that were originally there.
\begin{note}
This spurious failure enables implementation of compare-and-exchange
on a broader class of machines, e.g., load-locked store-conditional machines.
A consequence of spurious failure is
that nearly all uses of weak compare-and-exchange will be in a loop.
When a compare-and-exchange is in a loop,
the weak version will yield better performance on some platforms.
When a weak compare-and-exchange would require a loop and
a strong one would not, the strong one is preferable.
\end{note}
\end{itemdescr}

~\\
~\\

\textbf{\textit{Make the following changes in [atomics.ref.int].}} \\

\begin{codeblock}
namespace std {
  template<@\added{Lock}@> struct atomic_ref<@\placeholder{integral}\added{, Lock}@> {
  private:
    @\placeholder{integral}@* ptr;        // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = @\placeholder{integral}@;
    using difference_type = value_type;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(T&) noexcept;
    @\added{atomic_ref(\placeholder{integral}\&, atomic_ref_assume_lock_free_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{integral}\&, Lock\&) noexcept;}@
    @\added{atomic_ref(\placeholder{integral}\&, Lock\&, atomic_ref_prefer_user_lock_t) noexcept;}@
    
    @\added{\tcode{template<class Select> atomic_ref(\placeholder{integral}\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{Select = \unspec ) noexcept;}}@
    @\added{\tcode{template<class Select> atomic_ref(\placeholder{integral}\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{atomic_ref_prefer_user_lock_t, Select = \unspec ) noexcept;}}@

    atomic_ref(const atomic_ref&) noexcept;

    @\placeholdernc{integral}@ operator=(@\placeholder{integral}@) const noexcept;
    operator @\placeholdernc{integral}@() const noexcept;

    bool is_lock_free() const noexcept;
    void store(@\placeholdernc{integral}@, memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ load(memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ exchange(@\placeholdernc{integral}@,
                      memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order = memory_order_seq_cst) const noexcept;

    @\placeholdernc{integral}@ fetch_add(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_sub(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_and(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_or(@\placeholdernc{integral}@,
                      memory_order = memory_order_seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_xor(@\placeholdernc{integral}@,
                       memory_order = memory_order_seq_cst) const noexcept;

    @\placeholdernc{integral}@ operator++(int) const noexcept;
    @\placeholdernc{integral}@ operator--(int) const noexcept;
    @\placeholdernc{integral}@ operator++() const noexcept;
    @\placeholdernc{integral}@ operator--() const noexcept;
    @\placeholdernc{integral}@ operator+=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator-=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator&=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator|=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator^=(@\placeholdernc{integral}@) const noexcept;
  };
}
\end{codeblock}

\textbf{\textit{Make the following changes in [atomics.ref.float].}} \\

\begin{codeblock}
namespace std {
  template<@\added{Lock}@> struct atomic_ref<@\placeholder{floating-point}\added{, Lock}@> {
  private:
    @\placeholder{floating-point}@* ptr;  // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = @\placeholder{floating-point}@;
    using difference_type = value_type;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(T&) noexcept;
    @\added{atomic_ref(\placeholder{floating-point}\&, atomic_ref_assume_lock_free_t) noexcept;}@
    
    @\added{atomic_ref(\placeholder{floating-point}\&, Lock\&) noexcept;}@
    @\added{atomic_ref(\placeholder{floating-point}\&, Lock\&, atomic_ref_prefer_user_lock_t) noexcept;}@
    
    @\added{\tcode{template<class Select> atomic_ref(\placeholder{floating-point}\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{Select = \unspec ) noexcept;}}@
    @\added{\tcode{template<class Select> atomic_ref(\placeholder{floating-point}\&, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{atomic_ref_prefer_user_lock_t, Select = \unspec ) noexcept;}}@

    atomic_ref(const atomic_ref&) noexcept;

    @\placeholder{floating-point}@ operator=(@\placeholder{floating-point}@) noexcept;
    operator @\placeholdernc{floating-point}@() const noexcept;

    bool is_lock_free() const noexcept;
    void store(@\placeholder{floating-point}@, memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ load(memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ exchange(@\placeholder{floating-point}@,
                            memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholder{floating-point}@,
                                 memory_order = memory_order_seq_cst) const noexcept;

    @\placeholder{floating-point}@ fetch_add(@\placeholder{floating-point}@,
                             memory_order = memory_order_seq_cst) const noexcept;
    @\placeholder{floating-point}@ fetch_sub(@\placeholder{floating-point}@,
                             memory_order = memory_order_seq_cst) const noexcept;

    @\placeholder{floating-point}@ operator+=(@\placeholder{floating-point}@) const noexcept;
    @\placeholder{floating-point}@ operator-=(@\placeholder{floating-point}@) const noexcept;
  };
}
\end{codeblock}

\textbf{\textit{Make the following changes in [atomics.ref.pointer].}} \\

\begin{codeblock}
namespace std {
  template<class T@\added{, Lock}@> struct atomic_ref<T*@\added{, Lock}@> {
  private:
    T** ptr;              // \expos
    @\added{Lock* ulock;  // \expos}@
  public:
    using value_type = T*;
    using difference_type = ptrdiff_t;
    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    @\added{static constexpr bool never_requires_user_lock = \impdefx{};}@
    static constexpr size_t required_@\added{lock_free}_@alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    @\added{\tcode{static bool is_lock_free(const T \&) noexcept;}}@
    @\added{\tcode{static bool requires_user_lock(const T \&) noexcept;}}@

    atomic_ref& operator=(const atomic_ref&) = delete;

    explicit atomic_ref(T&) noexcept;
    @\added{atomic_ref(T*, atomic_ref_assume_lock_free_t) noexcept;}@
   
    @\added{atomic_ref(T*, Lock\&) noexcept;}@
    @\added{atomic_ref(T*, Lock\&, atomic_ref_prefer_user_lock_t) noexcept;}@
    
    @\added{\tcode{template<class Select> atomic_ref(T*, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{Select = \unspec ) noexcept;}}@
    @\added{\tcode{template<class Select> atomic_ref(T*, ranges::RandomAccessRange<Lock>, }}@
    @  \added{\tcode{atomic_ref_prefer_user_lock_t, Select = \unspec ) noexcept;}}@

    atomic_ref(const atomic_ref&) noexcept;

    T* operator=(T*) const noexcept;
    operator T*() const noexcept;

    bool is_lock_free() const noexcept;
    void store(T*, memory_order = memory_order_seq_cst) const noexcept;
    T* load(memory_order = memory_order_seq_cst) const noexcept;
    T* exchange(T*, memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order = memory_order_seq_cst) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order = memory_order_seq_cst) const noexcept;

    T* fetch_add(difference_type, memory_order = memory_order_seq_cst) const noexcept;
    T* fetch_sub(difference_type, memory_order = memory_order_seq_cst) const noexcept;

    T* operator++(int) const noexcept;
    T* operator--(int) const noexcept;
    T* operator++() const noexcept;
    T* operator--() const noexcept;
    T* operator+=(difference_type) const noexcept;
    T* operator-=(difference_type) const noexcept;
  };
}
\end{codeblock}

